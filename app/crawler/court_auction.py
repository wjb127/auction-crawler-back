from typing import List, Dict
from datetime import datetime, date
import re
from .base import BaseCrawler


class CourtAuctionCrawler(BaseCrawler):
    """ëŒ€ë²•ì› ê²½ë§¤ì •ë³´ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        super().__init__()
        self.base_url = "http://www.courtauction.go.kr"
        self.search_url = f"{self.base_url}/RetrieveRealEstateDetailList.laf"
    
    async def crawl_items(self, pages: int = 3) -> List[Dict]:
        """ëŒ€ë²•ì› ê²½ë§¤ì •ë³´ í¬ë¡¤ë§"""
        items = []
        
        try:
            for page in range(1, pages + 1):
                print(f"ğŸ“„ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")
                
                # ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì„¤ì •
                params = {
                    'pageIndex': page,
                    'pageSize': 20,
                    'realEstateUsage': '',  # ìš©ë„ (ë¹ˆê°’ = ì „ì²´)
                    'roadNameAddress': '',  # ë„ë¡œëª…ì£¼ì†Œ
                    'appraisalValueMin': '',  # ìµœì†Œ ê°ì •ê°€
                    'appraisalValueMax': '',  # ìµœëŒ€ ê°ì •ê°€
                }
                
                soup = self.get_page(self.search_url, params)
                if not soup:
                    continue
                
                # ê²½ë§¤ ë¬¼ê±´ ëª©ë¡ ì¶”ì¶œ
                item_rows = soup.find_all('tr', {'class': ['Ltbllist', 'Ltbllist2']})
                
                for row in item_rows:
                    try:
                        item_data = self._extract_item_data(row)
                        if item_data:
                            items.append(item_data)
                    except Exception as e:
                        print(f"âŒ ë¬¼ê±´ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        continue
                
                print(f"âœ… í˜ì´ì§€ {page}: {len(item_rows)}ê°œ ë¬¼ê±´ ì²˜ë¦¬")
                
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        print(f"ğŸ¯ ì´ {len(items)}ê°œ ë¬¼ê±´ í¬ë¡¤ë§ ì™„ë£Œ")
        return items
    
    def _extract_item_data(self, row) -> Dict:
        """í…Œì´ë¸” í–‰ì—ì„œ ë¬¼ê±´ ë°ì´í„° ì¶”ì¶œ"""
        try:
            cells = row.find_all('td')
            if len(cells) < 8:
                return None
            
            # ë¬¼ê±´ ìƒì„¸ ë§í¬ ì¶”ì¶œ
            detail_link = cells[2].find('a')
            if not detail_link:
                return None
            
            href = detail_link.get('href', '')
            detail_url = f"{self.base_url}/{href}" if href else ""
            
            # ë¬¼ê±´ëª…/ì£¼ì†Œ
            title = self.clean_text(detail_link.get_text())
            
            # ê°ì •ê°€ ì¶”ì¶œ
            appraisal_text = self.clean_text(cells[4].get_text())
            appraisal_value = self.extract_number(appraisal_text)
            
            # ì…ì°°ì¼ ì¶”ì¶œ
            bid_date_text = self.clean_text(cells[6].get_text())
            bid_date = self._parse_date(bid_date_text)
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ (ì œëª©ì—ì„œ)
            keywords = self._extract_keywords(title)
            
            return {
                'title': title,
                'appraisal_value': appraisal_value,
                'bid_date': bid_date,
                'url': detail_url,
                'keywords': keywords,
                'source_site': 'ëŒ€ë²•ì› ê²½ë§¤ì •ë³´'
            }
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def _parse_date(self, date_text: str) -> date:
        """ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ date ê°ì²´ë¡œ ë³€í™˜"""
        try:
            # "2024.02.15" í˜•íƒœë¡œ ê°€ì •
            date_match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})', date_text)
            if date_match:
                year, month, day = map(int, date_match.groups())
                return date(year, month, day)
        except Exception:
            pass
        return None
    
    def _extract_keywords(self, title: str) -> List[str]:
        """ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # ì§€ì—­ í‚¤ì›Œë“œ
        regions = ['ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°', 'ì„¸ì¢…']
        districts = ['ê°•ë‚¨êµ¬', 'ê°•ì„œêµ¬', 'ì†¡íŒŒêµ¬', 'ì„œì´ˆêµ¬', 'ê°•ë™êµ¬', 'ë§ˆí¬êµ¬', 
                    'ìš©ì‚°êµ¬', 'ì¤‘êµ¬', 'ì¢…ë¡œêµ¬', 'ì„±ë™êµ¬', 'ë™ì‘êµ¬', 'ê´€ì•…êµ¬']
        
        # ë¬¼ê±´ ìœ í˜• í‚¤ì›Œë“œ
        property_types = ['ì•„íŒŒíŠ¸', 'ì˜¤í”¼ìŠ¤í…”', 'ìƒê°€', 'ì£¼íƒ', 'ë¹Œë¼', 'ì—°ë¦½ì£¼íƒ', 
                         'ë‹¨ë…ì£¼íƒ', 'í† ì§€', 'ê±´ë¬¼', 'ì‚¬ë¬´ì‹¤']
        
        for keyword_list in [regions, districts, property_types]:
            for keyword in keyword_list:
                if keyword in title:
                    keywords.append(keyword)
        
        return list(set(keywords))  # ì¤‘ë³µ ì œê±° 